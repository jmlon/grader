TITLE: AI-Assisted Grading Crew


# Introduction

The automated evaluation of student assignments presents a significant challenge in education. Traditional grading methods are labor-intensive, demanding considerable human effort and are susceptible to biases and inconsistencies arising from grader fatigue and subjective interpretations. This blog post introduces an AI-assisted grading system leveraging the CrewAI framework to address these limitations. By employing a multi-agent approach, we harness the power of Large Language Models (LLMs) to streamline the grading process, enhance objectivity, and provide more detailed feedback to students. CrewAI facilitates the decomposition of the complex grading task into manageable sub-tasks, each handled by specialized AI agents. This approach allows for the utilization of LLMs that are best suited for specific aspects of the evaluation, while also enabling the use of targeted prompts that ensure accurate and relevant assessments.

# Methodology

Our AI-assisted grading system is built upon the CrewAI framework, which enables the creation of collaborative multi-agent systems. The architecture comprises two distinct agents: a Grader and a Checker. The Grader agent is responsible for the initial evaluation of student submissions based on a predefined rubric. The Checker agent then reviews the Grader's assessment, ensuring accuracy, consistency, and alignment with the rubric's learning outcomes.

The grading process is divided into two primary tasks: Evaluation and Reviewing. The Evaluation task involves the Grader agent analyzing the student's submission, comparing it against the rubric, and assigning scores and feedback for each learning outcome. The Reviewing task is performed by the Checker agent, who examines the Grader's evaluation to verify the accuracy of the assigned scores and the quality of the feedback provided.

Both agents receive the following inputs: the assignment description, the grading rubric, and the student's submission. These inputs, combined with carefully crafted prompts, guide the LLMs in performing their respective tasks. The prompts are designed to provide clear instructions and context, ensuring that the agents focus on the key aspects of the assignment and the learning outcomes defined in the rubric.

For the evaluation of the system, we used programming assignments and their corresponding rubrics. We tested various combinations of LLMs to evaluate their performance in the different tasks. The results were evaluated by hand, by qualitatively reviewing the assigned grade and the review of the grade generated by the crew.

# Experiments


# Results and Discussion

